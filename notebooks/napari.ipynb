{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905d6643-87e7-4b6d-b21d-9d779b19cbab",
   "metadata": {},
   "source": [
    "# Examining the Ground Truth LBM Dataset\n",
    "\n",
    "Input: mh89_hemisphere_00001.tif -> mh89_hemisphere_00010.tif\n",
    "- [25320, 5104, 145] \n",
    "- [Zt, y, x]\n",
    "\n",
    "## 1) Pre-Processing / Motion Correction\n",
    "\n",
    "Output: Fig2_dataset_plane_n.mat, where n = 1:30, containing the following fields:\n",
    "\n",
    "- Y: motion-corrected single plane imaging data [nx,ny,nt]\n",
    "- pixelResolution: pixel sampling [um]\n",
    "- sizY: size of y [nx,ny,nt]\n",
    "- volumeRate: volume rate [Hz]\n",
    "\n",
    "## 2) Segmentation\n",
    "\n",
    "Output: Fig2_collated_caiman_output.mat, containing the following fields:\n",
    "\n",
    "- T_all: raw neuronal traces [K,nt]\n",
    "- nx: neuronal coordinate in the x direction [K,1], [um]\n",
    "- ny: neuronal coordinate in the y direction [K,1], [um]\n",
    "- nz: neuronal coordinate in the z direction [K,1], [um]"
   ]
  },
  {
   "cell_type": "code",
   "id": "1175393b1c00b405",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-04T14:50:31.240496Z",
     "start_time": "2024-03-04T14:50:31.213223Z"
    }
   },
   "source": [
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import scipy.io\n",
    "import dask\n",
    "import napari\n",
    "from pprint import pprint\n",
    "from skimage.io import imread\n",
    "from dask import delayed\n",
    "\n",
    "try:\n",
    "    from icecream import ic, install, argumentToString\n",
    "    install()\n",
    "except ImportError:  # graceful fallback if icecream isn't installed.\n",
    "    ic = lambda *a: None if not a else (a[0] if len(a) == 1 else a)  # noqa\n",
    "\n",
    "install()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b7dd13-67c2-4d13-b2bb-b1a496286189",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_imread = delayed(tifffile.imread)\n",
    "\n",
    "reader = lazy_imread('/data2/fpo/lbm/*.tif')  # doesn't actually read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8a2e6-eda0-436c-a66c-7c991537d19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fc9da-2bed-4c70-bb0e-29b05d40d25b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tifffile.imread(fname, as_zarr=True) -> Dask\n",
    "arr = da.imread(\"/data2/fpo/lbm/*.tif\", chunks=(1, 25320, 5104, 145))\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d987c8-9582-48c7-90eb-beb9d5a12d39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T22:50:02.107487Z",
     "start_time": "2024-02-28T22:50:02.020803Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "do_metadata = False\n",
    "\n",
    "if do_metadata:\n",
    "    json_filename = \"/v-data4/foconnell/data.json\"\n",
    "    try:\n",
    "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(serialized_metadata)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occured: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c3514-83aa-4e61-a266-e36d10d121e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "arr = arr.rechunk(1, 30, 5104, 145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "909b7e7a-b452-49d6-9791-e34e15fb65e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 25320, 5104, 145)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "figname = \"/v-data4/foconnell/figA.png\"\n",
    "\n",
    "print(arr.shape)\n",
    "#slice = arr[1, 1, :1021, :].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ef756-78ae-416a-9016-89235057881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "ax.imshow(arr[1, 1, :, :])\n",
    "#ax[0,1].imshow(tiled_astronaut_images[1])\n",
    "#ax[1,0].imshow(tiled_astronaut_images[2])\n",
    "#ax[1,1].imshow(tiled_astronaut_images[3])\n",
    "#ax.imshow(arr.compute(), cmap=\"gray\")\n",
    "plt.gcf()\n",
    "#plt.savefig(figname)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da588993-45c8-49dd-93a6-9bda6bc3f6a9",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "ScanImage metadata via tags and reader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38175531-17f3-44cb-bb54-87bda7166c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tifffile.TiffFile(p[0]) as tif:\n",
    "    metadata = {}\n",
    "    for tag in tif.pages[0].tags.values():\n",
    "        tag_name, tag_value = tag.name, tag.value\n",
    "        metadata[tag_name] = tag_value\n",
    "        \n",
    "metadims = {}\n",
    "for k, v in metadata.items():\n",
    "    if v in ['145', '144', 145, 144, 5104, '5104']:\n",
    "        metadims[k] = v\n",
    "        \n",
    "pprint(metadims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f875f5-77b4-4562-803b-acb29f28eaac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:00:56.463112003Z",
     "start_time": "2024-02-16T01:00:56.441175908Z"
    }
   },
   "outputs": [],
   "source": [
    "chans_order = np.array([ 1,  5,  6,  7,  8,  9,  2, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "                            3, 18, 19, 20, 21, 22, 23,  4, 24, 25, 26, 27, 28, 29, 30]) - 1\n",
    "n_planes = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb020a6-98d5-4141-8a6f-bbfd8c74fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_rarr = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb9adf-d9fb-4e60-bb5a-5ada5256fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import h5py\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# Define chunk sizes to test\n",
    "chunk_sizes = [32 * 1024, 64 * 1024, 128 * 1024, 256 * 1024, 512 * 1024]  # in bytes\n",
    "\n",
    "# Metrics to collect\n",
    "creation_times = []\n",
    "file_sizes = []\n",
    "sequential_read_times = []\n",
    "random_read_times = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    # Calculate chunk dimensions\n",
    "    chunk_dims = (chunk_size // data.dtype.itemsize // data.shape[1], data.shape[1])\n",
    "    \n",
    "    # Create HDF5 file with specified chunk size and compression\n",
    "    h5_file_path = f'temp_{chunk_size}.h5'\n",
    "    start_time = time.time()\n",
    "    with h5py.File(h5_file_path, 'w') as f:\n",
    "        dset = f.create_dataset('data', data=data, compression='gzip', chunks=chunk_dims)\n",
    "    creation_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Measure file size\n",
    "    file_sizes.append(os.path.getsize(h5_file_path) / (1024 * 1024))  # Convert to MB\n",
    "    \n",
    "    # Sequential read\n",
    "    start_time = time.time()\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        _ = f['data'][:]\n",
    "    sequential_read_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Random read\n",
    "    start_time = time.time()\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        for _ in range(100):\n",
    "            index = np.random.randint(0, data.shape[0])\n",
    "            _ = f['data'][index, :]\n",
    "    random_read_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(h5_file_path)\n",
    "\n",
    "# Plotting results\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs[0, 0].plot(chunk_sizes, creation_times, marker='o')\n",
    "axs[0, 0].set_title('Creation Time vs. Chunk Size')\n",
    "axs[0, 0].set_xlabel('Chunk Size (bytes)')\n",
    "axs[0, 0].set_ylabel('Time (s)')\n",
    "\n",
    "axs[0, 1].plot(chunk_sizes, file_sizes, marker='o')\n",
    "axs[0, 1].set_title('File Size vs. Chunk Size')\n",
    "axs[0, 1].set_xlabel('Chunk Size (bytes)')\n",
    "axs[0, 1].set_ylabel('Size (MB)')\n",
    "\n",
    "axs[1, 0].plot(chunk_sizes, sequential_read_times, marker='o')\n",
    "axs[1, 0].set_title('Sequential Read Time vs. Chunk Size')\n",
    "axs[1, 0].set_xlabel('Chunk Size (bytes)')\n",
    "axs[1, 0].set_ylabel('Time (s)')\n",
    "\n",
    "axs[1, 1].plot(chunk_sizes, random_read_times, marker='o')\n",
    "axs[1, 1].set_title('Random Read Time vs. Chunk Size')\n",
    "axs[1, 1].set_xlabel('Chunk Size (bytes)')\n",
    "axs[1, 1].set_ylabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
