{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d067b409-a6b6-4cf0-9651-13535329f02d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T04:09:21.951898Z",
     "iopub.status.busy": "2024-05-02T04:09:21.951329Z",
     "iopub.status.idle": "2024-05-02T04:09:22.081278Z",
     "shell.execute_reply": "2024-05-02T04:09:22.080475Z",
     "shell.execute_reply.started": "2024-05-02T04:09:21.951834Z"
    }
   },
   "source": [
    "# Light Beads Microscopy Demo Pipeline \n",
    "\n",
    "## Overview\n",
    "### Pre-Processing:\n",
    "- Extract ScanImage metadata\n",
    "- Correct Bi-Directional Offset for each ROI\n",
    "- Calculates and corrects the MROI seams (IN PROGRESS)\n",
    "### Motion Correction\n",
    "- Apply the nonrigid motion correction (NoRMCorre) algorithm for motion correction.\n",
    "- View pre/most correction movie\n",
    "- Use quality metrics to evaluate registration quality\n",
    "### Segmentation\n",
    "- Apply the constrained nonnegative matrix factorization (CNMF) source separation algorithm to extract initial estimates of neuronal spatial footprints and calcium traces.\n",
    "- Apply quality control metrics to evaluate the initial estimates, and narrow down to the final set of estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb1f45-43a2-463c-8924-b92519434c37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T04:09:21.951898Z",
     "iopub.status.busy": "2024-05-02T04:09:21.951329Z",
     "iopub.status.idle": "2024-05-02T04:09:22.081278Z",
     "shell.execute_reply": "2024-05-02T04:09:22.080475Z",
     "shell.execute_reply.started": "2024-05-02T04:09:21.951834Z"
    }
   },
   "source": [
    "### Setup\n",
    "- Import necessary libraries\n",
    "Notable: Numpy, Cv2, Zarr, Dask, Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63dc97d-3aa9-4955-8bac-9d67ed6fc427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T02:20:04.136311883Z",
     "start_time": "2024-05-02T02:20:03.879794240Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-07T04:04:08.885695Z",
     "iopub.status.busy": "2024-05-07T04:04:08.884740Z",
     "iopub.status.idle": "2024-05-07T04:04:09.614305Z",
     "shell.execute_reply": "2024-05-07T04:04:09.596318Z",
     "shell.execute_reply.started": "2024-05-07T04:04:08.885621Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "# Give this notebook access to the root package\n",
    "sys.path.append('../../')  # TODO: Take this out when we upload to pypi\n",
    "print(sys.path[0])\n",
    "\n",
    "import core.io\n",
    "import scanreader\n",
    "\n",
    "import zarr\n",
    "try:\n",
    "    import dask.array as da\n",
    "except:\n",
    "    %pip install dask\n",
    "    import dask.array as da\n",
    "import bokeh.plotting as bpl\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "from IPython import get_ipython\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except():\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "        get_ipython().run_line_magic('autoreload', '2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "bpl.output_notebook()\n",
    "hv.notebook_extension('bokeh', 'matplotlib')\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(format=\"{asctime} - {levelname} - [{filename} {funcName}() {lineno}] - pid {process} - {message}\",\n",
    "                    filename=None, \n",
    "                    level=logging.WARNING, style=\"{\") # this shows you just errors that can harm your program\n",
    "                    # level=logging.DEBUG, style=\"{\") # this shows you general information that developers use to trakc their program \n",
    "                    # (be careful when playing movies, there will be a lot of debug messages)\n",
    "\n",
    "# set env variables \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2c42cf28",
   "metadata": {},
   "source": [
    "#### Set up a few helper functions for plotting, logging and setting up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc06dec9-432e-45ea-baf1-3448f2336b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T00:07:14.747040Z",
     "iopub.status.busy": "2024-05-07T00:07:14.746521Z",
     "iopub.status.idle": "2024-05-07T00:07:14.883893Z",
     "shell.execute_reply": "2024-05-07T00:07:14.882937Z",
     "shell.execute_reply.started": "2024-05-07T00:07:14.746979Z"
    }
   },
   "source": [
    "chan_order = np.array([ 1, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14, 15, 16, 17, 3, 18, 19, 20, 21, 22, 23, 4, 24, 25, 26, 27, 28, 29, 30])  # this is specific to our dataset\n",
    "chan_order = [x-1 for x in chan_order]  # convert to 0-based indexing"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "03dbfff6-d7d8-4196-80d6-7ce80ebc25a8",
   "metadata": {},
   "source": [
    "## Extract data using scanreader, joining contiguous ROI's, and plot our mean image\n",
    "\n",
    "Our ScanReader object contains all of the properties needed to keep track of our raw data. \n",
    "- ScanImage metadata is stored alongside header metadata, this ScanImage specific data is what's needed to assemble frames from constituent ROIs.\n",
    "- We calculate the frame rate and time/pixels between scans and ROI's using the following metadata:\n",
    "\n",
    "![frame rate calculation](../../docs/img/FrameRate1eq.png)\n",
    "\n",
    "\n",
    "#### Joining Contiguious ROI's\n",
    "\n",
    "Setting `join_contiguous=True` will combine ROI's with the following constraints:\n",
    "\n",
    "1) Must be the same size/shape\n",
    "2) Must be located in the same scanning depth\n",
    "3) Must be located in the same slice\n",
    "- ROI can be directly left, right, above or below the adjacent ROI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0896dd84-83cf-4a4c-916d-5aafc4c29bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T00:07:22.177384Z",
     "iopub.status.busy": "2024-05-07T00:07:22.176689Z",
     "iopub.status.idle": "2024-05-07T00:07:22.917568Z",
     "shell.execute_reply": "2024-05-07T00:07:22.916705Z",
     "shell.execute_reply.started": "2024-05-07T00:07:22.177309Z"
    }
   },
   "source": [
    "datapath = Path('/data2/fpo/data/')                 # string pointing to directory containing your data\n",
    "htiffs = [x for x in datapath.glob('*.tif')]         # this accumulates a list of every filepath which contains a .tif file\n",
    "\n",
    "reader = scanreader.read_scan(str(htiffs[0]), join_contiguous=True, lbm=True, x_cut=6)  # this should take < 2s, no actual data is being read yet\n",
    "print(f\"Number of Planes: {reader.num_channels}\")\n",
    "print(f\"Number of ROIs: {reader.num_fields}\")\n",
    "print(f\"Total frames (single .tiff) {reader.num_frames}\")\n",
    "print(f\"Total frames (all .tiffs) {reader.num_requested_frames}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc4996a",
   "metadata": {},
   "source": [
    "datapath = Path('/data2/fpo/data')                 # string pointing to directory containing your data\n",
    "savepath = Path('/data2/fpo/data/temp/')                 # string pointing to directory containing your data\n",
    "savepath.mkdir(exist_ok=True, parents=True)\n",
    "htiffs = [x for x in datapath.glob('*.tif')]         # this accumulates a list of every filepath which contains a .tif file\n",
    "\n",
    "times = []\n",
    "times2 = []\n",
    "start = time.time()\n",
    "for i in range(1, 10):\n",
    "    reader = scanreader.read_scan(str(htiffs[0]), join_contiguous=True, lbm=True, x_cut=i)  # this should take < 2s, no actual data is being read yet\n",
    "    data = reader[0]\n",
    "    save_str = savepath / f'xcut_{i}.zarr'\n",
    "    store = zarr.DirectoryStore(save_str)  # save data to persistent disk storage\n",
    "    try:\n",
    "        z = zarr.zeros(data.shape, chunks=(data.shape[0], data.shape[1], 1, data.shape[3]), dtype='int16', store=store, overwrite=False)\n",
    "    except:\n",
    "        continue\n",
    "    z[:] = data             # this will auto-chunk based on the specified chunks in 'open'\n",
    "    times.append( f\"{time.time() - start:.2f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1aa9bf",
   "metadata": {},
   "source": [
    "tmp = [x for x in Path(savepath).glob('*.zarr')]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63b9e985",
   "metadata": {},
   "source": [
    "# get each time relative to the start time \n",
    "durations = [x - times[0] for x in times]\n",
    "labels = [f'pixels cut: {i}' for i in range(1, 10)]\n",
    "pd.DataFrame({'Length of write': durations}, index=labels) #.to_csv(savepath / 'xcut_times.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c71854c",
   "metadata": {},
   "source": [
    "arr.info"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b894d5",
   "metadata": {},
   "source": [
    "digits = [int(x.stem.split('_')[-1]) for x in Path(savepath).glob('*.zarr')]\n",
    "\n",
    "images = []\n",
    "for i in digits:\n",
    "    arr = zarr.open(str(savepath / f'xcut_{i}.zarr'), mode='r')\n",
    "    images.append(hv.Image(arr[:,:,5,400]).opts(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        title=f\"pixels_cut: {i}\",\n",
    "        tools=['wheel_zoom'],\n",
    "        cmap='gray', \n",
    "        )\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf74145",
   "metadata": {},
   "source": [
    "images"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "681e8c2b",
   "metadata": {},
   "source": [
    "@pn.cache\n",
    "def get_data(i):\n",
    "    fullpath = Path(savepath) / f'xcut_{i}.zarr'\n",
    "    if not fullpath.exists():\n",
    "        print(f\"File {fullpath} does not exist. Skipping...\")\n",
    "        return None\n",
    "    arr = zarr.open(str(fullpath), mode='r')\n",
    "    return arr[:,:,5,400]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41a2828f",
   "metadata": {},
   "source": [
    "def get_plot(i1,i2):\n",
    "    data1 = get_data(i1)\n",
    "    data2 = get_data(i2)\n",
    "    img1 = hv.Image(data1).opts(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        title=f\"pixels_cut: {i1}\",\n",
    "        tools=['wheel_zoom'],\n",
    "        cmap='gray', \n",
    "    )\n",
    "    img2 = hv.Image(data2).opts(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        title=f\"pixels_cut: {i2}\",\n",
    "        tools=['wheel_zoom'],\n",
    "        cmap='gray', \n",
    "    )\n",
    "    return img1 + img2\n",
    "\n",
    "# # Widgets\n",
    "image_widget_1 = pn.widgets.IntSlider(name=\"Image_widget_1\", value=5, start=5, end=9)\n",
    "image_widget_2 = pn.widgets.IntSlider(name=\"Image_widget_2\", value=6, start=6, end=9)\n",
    "\n",
    "# # Binding the plot function to the widgets\n",
    "bound_plot = pn.bind(get_plot, i1=image_widget_1, i2=image_widget_2) \n",
    "\n",
    "# # Layout of widgets and plot\n",
    "layout = pn.Column(\n",
    "    pn.Row(image_widget_1, image_widget_2, sizing_mode=\"fixed\", width=300),\n",
    "    bound_plot\n",
    ")\n",
    "\n",
    "# Display the layout\n",
    "layout.servable()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09d10ce",
   "metadata": {},
   "source": [
    "pn.template.MaterialTemplate(\n",
    "    site=\"Panel\",\n",
    "    title=\"Getting Started App\",\n",
    "    sidebar=[image_widget_1, image_widget_2],\n",
    "    main=[bound_plot],\n",
    ").servable(); "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1234ae72",
   "metadata": {},
   "source": [
    "# iterate over each tiff page to collect offsets. 40s for [300x300x30x1700] dtype=int16\n",
    "data = reader[0]\n",
    "print(f'Data shape: {data.shape} \\nData type: {data.dtype}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4fa529da",
   "metadata": {},
   "source": [
    "xd"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d6ea6ec5",
   "metadata": {},
   "source": [
    "### Data Storage: Zarr\n",
    "\n",
    "[Zarr documentation](https://zarr.readthedocs.io/en/stable/tutorial.html)\n",
    "\n",
    "Deciding how to save data on a host operating system is *far* from straight foreward. Read/write operations will vary widely between data saved in a **single file**\n",
    "structure vs smaller chunks, e.g. one image per file, one image per epoch, etc. \n",
    " \n",
    "The former strategy is clean/consice and easy to handle but is *not* feasable with large (>10GB) datasets. \n",
    "\n",
    "The latter strategy of spreading files acrossed nested groups of directories, each with their own metadata/attributes has been widely adopted as the more sensible approach. HDF5 has \n",
    "been the frontrunner in scientific data I/O but suffers from widely inconsistent within academia.  \n",
    "\n",
    "- Zarr, similar to H5, is a heirarchical data storage specification (or in non-alien speak: \"rules of how data is stored on disk\").\n",
    "- Zarr nicely hides the complexities inherent in linking filesystem heirarchy with efficient data I/O.\n",
    "\n",
    "#### The below section demonstrates how to search for the `optimal` data chunking/partitioning scheme for our datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ec12ca9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T15:50:41.914458Z",
     "start_time": "2024-05-07T15:50:41.833759Z"
    }
   },
   "source": [
    "def process_dataset(data, dataset_name, chunk_shape, savepath=''):\n",
    "    savepath = Path(savepath).with_suffix('.zarr')\n",
    "    if not savepath.exists():\n",
    "        print(f\"{savepath} doesn't exist, creating...\")\n",
    "        savepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # benchmark write\n",
    "    start = time.time()\n",
    "    store = zarr.DirectoryStore(savepath)  # save data to persistent disk storage\n",
    "    z = zarr.zeros(data.shape, chunks=chunk_shape, dtype='int16', store=store, overwrite=True)\n",
    "\n",
    "    z[:] = data             # this will auto-chunk based on the specified chunks in 'open'\n",
    "    # z = data              # equivalent \n",
    "    # z[:, :, :, :] = data  # equivalent\n",
    "\n",
    "    write = time.time() - start\n",
    "    formatted_write = f\"{write:.2f}\"\n",
    "\n",
    "    # benchmark read\n",
    "    start = time.time()\n",
    "    _ = z[:]\n",
    "    read = time.time() - start\n",
    "    formatted_read = f\"{read:.2f}\"\n",
    "\n",
    "    chunksize_nbytes = np.prod(chunk_shape) * z.dtype.itemsize  # 2 bytes per int16\n",
    "    return [\n",
    "        str(data.shape),\n",
    "        str(z.chunks),\n",
    "        z.nbytes / 1e6,\n",
    "        chunksize_nbytes / 1e6,\n",
    "        z.dtype,\n",
    "        z.order,\n",
    "        formatted_read,\n",
    "        formatted_write,\n",
    "        z.store.path\n",
    "    ]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c3ec08f1",
   "metadata": {},
   "source": [
    "### Save our raw data to a zarr store\n",
    "\n",
    "Zarr allows us to maintain persistent storage, that is, save chunks of our data to disk without needing to hold it in memory.\n",
    "\n",
    "- `zarr.open` is a convenience method that handles chunking and compression for persistant storage.\n",
    "- We want to keep this data as `16 bit` integers because no calculations should be done yet.\n",
    "\n",
    "In general, we want this value to be `~1Mb` to optimize write speed. \n",
    "\n",
    "```python\n",
    "name = '/path/to/folder'\n",
    "chunksize=[300,,30]\n",
    "z1 = zarr.open(f'{name}', mode='w', shape=(data.shape),chunks=chunksize, dtype='int16')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cdb05825",
   "metadata": {},
   "source": [
    "labels = [\n",
    "    'Array  [x,y,z,t]',\n",
    "    'Chunks [x,y,z,t]',\n",
    "    'Array Size (Mb)',\n",
    "    'Chunk Size (Mb)',\n",
    "    'Data Type',\n",
    "    'Order',\n",
    "    'Read Time (s)',\n",
    "    'Write Time (s)',\n",
    "    'Save Path'\n",
    "]\n",
    "\n",
    "# the chunk sizes we want to benchmark\n",
    "chunksizes = [\n",
    "    (data.shape[0], data.shape[1], 1, data.shape[3]),  # [300x300x1x1750]\n",
    "    (data.shape[0], data.shape[1], data.shape[2], 1),  # [300x300x30x1  ]\n",
    "    (data.shape[0], data.shape[1], 1, 1)               # [300x300x1x1   ]\n",
    "]\n",
    "\n",
    "# give our dataset a name. this will be the column header\n",
    "names = [\n",
    "    ('chunked_by_plane'),  # [300x300x1x1750]\n",
    "    ('chunked_by_frame'),  # [300x300x30x1  ]\n",
    "    ('chunked_by_image')   # [300x300x1x1   ]\n",
    "]\n",
    "\n",
    "vals = {data_name: [] for data_name in names}\n",
    "for i, (chunksize, dataset) in enumerate(zip(chunksizes, names)):\n",
    "    # save the same data but with different chunk sizes\n",
    "    vals[dataset] = process_dataset(data, dataset, chunksize, savepath=f\"/data2/fpo/data/zarr/{dataset}\")\n",
    "\n",
    "df = pd.DataFrame(index=labels, columns=names, data=None)\n",
    "for k, v in vals.items():\n",
    "    df[k] = v\n",
    "df\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "de5218c8",
   "metadata": {},
   "source": [
    "## Inspect raw vs extracted zarr data \n",
    "Visual inspection to verify the chosen compression didn't lead to any artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5ba1f522",
   "metadata": {},
   "source": [
    "images = []\n",
    "image_raw = hv.Image(data[:, :, 5, 400]).opts(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        title=\"Raw Image\",\n",
    "        tools=['wheel_zoom'],\n",
    "        cmap='gray', \n",
    "        )\n",
    "for dname in df.columns:\n",
    "    zpath = df.loc['Save Path', dname]\n",
    "    arr = zarr.open(zpath, mode='r')\n",
    "    images.append(hv.Image(arr[:,:,5,400]).opts(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        title=dname,\n",
    "        tools=['wheel_zoom'],\n",
    "        cmap='gray', \n",
    "        )\n",
    "    )\n",
    "\n",
    "# layout =image_raw+ images[0] + images[1] + images[2]\n",
    "bpl.show(hv.render(image_raw))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "039fdb8d-75fb-4242-9b74-b28ddcabeb54",
   "metadata": {},
   "source": [
    "## Scan Phase Correction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### *Methods:*\n",
    "\n",
    "**1) Linear interpolation**\n",
    "\n",
    "**2) Phase - cross correlation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971200fe",
   "metadata": {},
   "source": [
    "### Phase correction via Linear Phase Interpolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7292fdf3-547e-4736-a335-8eee95550a61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T03:16:47.659242Z",
     "iopub.status.busy": "2024-05-07T03:16:47.658589Z",
     "iopub.status.idle": "2024-05-07T03:16:58.266716Z",
     "shell.execute_reply": "2024-05-07T03:16:58.265895Z",
     "shell.execute_reply.started": "2024-05-07T03:16:47.659173Z"
    }
   },
   "source": [
    "\n",
    "xd = da.from_array(z1, chunks=z1.chunks)\n",
    "xd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dffe34-aa68-4074-88a3-80788bba4209",
   "metadata": {
    "execution": {
     "execution_failed": "2024-05-07T04:19:13.976Z",
     "iopub.execute_input": "2024-05-07T04:04:22.365159Z",
     "iopub.status.busy": "2024-05-07T04:04:22.363984Z"
    }
   },
   "source": [
    "phase_angle = core.util.compute_raster_phase(xd[:,:, 5, 400], reader.temporal_fill_fraction)\n",
    "corrected_li = core.util.correct_raster(xd, phase_angle, reader.temporal_fill_fraction)\n",
    "phase_angle"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bb5a902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T02:30:33.624014Z",
     "iopub.status.busy": "2024-05-07T02:30:33.623380Z",
     "iopub.status.idle": "2024-05-07T02:32:15.688782Z",
     "shell.execute_reply": "2024-05-07T02:32:15.687213Z",
     "shell.execute_reply.started": "2024-05-07T02:30:33.623946Z"
    }
   },
   "source": [
    "corr = core.util.return_scan_offset(slice_plane[:,:,400], 1)\n",
    "corrected_pc = core.util.fix_scan_phase(z1, corr, 1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b2326ace-0771-49f2-b0ee-79d0eb5ec20a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee6d0d64-8584-43ec-bfa7-51e06604567d",
   "metadata": {},
   "source": [
    "## Motion Correction: CaImAn - NORMCorre\n",
    "\n",
    "### Load pre-processed data as a CaImAn `movie`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bd042",
   "metadata": {},
   "source": [
    "import caiman as cm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e7697-a805-43b3-bda7-69d216ace4dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T04:23:21.855414Z",
     "iopub.status.busy": "2024-05-02T04:23:21.855073Z",
     "iopub.status.idle": "2024-05-02T04:23:21.907283Z",
     "shell.execute_reply": "2024-05-02T04:23:21.906507Z",
     "shell.execute_reply.started": "2024-05-02T04:23:21.855390Z"
    }
   },
   "source": [
    "movie = cm.movie(slice_plane, start_time=2, fr=reader.fps)\n",
    "downsampling_ratio = 0.2  # subsample 5x\n",
    "movie = movie.resize(fz=downsampling_ratio)\n",
    "# movie.play(gain=1.3, backend='embed_opencv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "25b14011",
   "metadata": {},
   "source": [
    "### View correlation metrics\n",
    "\n",
    "Create a couple of summary images of the movie, including:\n",
    "- maximum projection (the maximum value of each pixel) \n",
    "- correlation image (how correlated each pixel is with its neighbors)\n",
    "\n",
    "If a pixel comes from an active neural component it will tend to be highly correlated with its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5be6cf",
   "metadata": {},
   "source": [
    "max_projection_orig = np.max(movie, axis=0)\n",
    "correlation_image_orig = cm.local_correlations(movie, swap_dim=False)\n",
    "correlation_image_orig[np.isnan(correlation_image_orig)] = 0 # get rid of NaNs, if they exist"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01051dd",
   "metadata": {},
   "source": [
    "f, (ax_max, ax_corr) = plt.subplots(1,2)\n",
    "ax_max.imshow(max_projection_orig, \n",
    "              cmap='viridis',\n",
    "              vmin=np.percentile(np.ravel(max_projection_orig),50), \n",
    "              vmax=np.percentile(np.ravel(max_projection_orig),99.5));\n",
    "ax_max.set_title(\"Max Projection Orig\", fontsize=12);\n",
    "\n",
    "ax_corr.imshow(correlation_image_orig, \n",
    "               cmap='viridis', \n",
    "               vmin=np.percentile(np.ravel(correlation_image_orig),50), \n",
    "               vmax=np.percentile(np.ravel(correlation_image_orig),99.5));\n",
    "ax_corr.set_title('Correlation Image Orig', fontsize=12);"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a226c084",
   "metadata": {},
   "source": [
    "### Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f9a14",
   "metadata": {},
   "source": [
    "max_shifts = (6, 6)  # maximum allowed rigid shift in pixels (view the movie to get a sense of motion)\n",
    "strides =  (48, 48)  # create a new patch every x pixels for pw-rigid correction\n",
    "overlaps = (24, 24)  # overlap between patches (size of patch strides+overlaps)\n",
    "max_deviation_rigid = 3   # maximum deviation allowed for patch with respect to rigid shifts\n",
    "pw_rigid = False  # flag for performing rigid or piecewise rigid motion correction\n",
    "shifts_opencv = True  # flag for correcting motion using bicubic interpolation (otherwise FFT interpolation is used)\n",
    "border_nan = 'copy'  # replicate values along the boundary (if True, fill in with NaN)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529e518",
   "metadata": {},
   "source": [
    "parameter_dict = {'fnames': tiffs,\n",
    "                  'fr': fr,\n",
    "                  'dxy': dxy,\n",
    "                  'decay_time': decay_time,\n",
    "                  'strides': strides,\n",
    "                  'overlaps': overlaps,\n",
    "                  'max_shifts': max_shifts,\n",
    "                  'max_deviation_rigid': max_deviation_rigid,\n",
    "                  'pw_rigid': pw_rigid,\n",
    "                  'p': p,\n",
    "                  'nb': gnb,\n",
    "                  'rf': rf,\n",
    "                  'K': K, \n",
    "                  'gSig': gSig,\n",
    "                  'gSiz': gSiz,\n",
    "                  'stride': stride_cnmf,\n",
    "                  'method_init': method_init,\n",
    "                  'rolling_sum': True,\n",
    "                  'only_init': True,\n",
    "                  'ssub': ssub,\n",
    "                  'tsub': tsub,\n",
    "                  'merge_thr': merge_thr, \n",
    "                  'bas_nonneg': bas_nonneg,\n",
    "                  'min_SNR': min_SNR,\n",
    "                  'rval_thr': rval_thr,\n",
    "                  'use_cnn': True,\n",
    "                  'min_cnn_thr': cnn_thr,\n",
    "                  'cnn_lowest': cnn_lowest}\n",
    "\n",
    "parameters = params.CNMFParams(params_dict=parameter_dict) # CNMFParams is the parameters class\n",
    "print(f\"You have {psutil.cpu_count()} CPUs available in your current environment\")\n",
    "num_processors_to_use = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bdfd7c81",
   "metadata": {},
   "source": [
    "#%% start the cluster (if a cluster already exists terminate it)\n",
    "if 'dview' in locals():\n",
    "    cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='multiprocessing', n_processes=None, single_thread=False)\n",
    "# create a motion correction object\n",
    "\n",
    "mc = MotionCorrect(fnames, dview=dview, max_shifts=max_shifts,\n",
    "                  strides=strides, overlaps=overlaps,\n",
    "                  max_deviation_rigid=max_deviation_rigid, \n",
    "                  shifts_opencv=shifts_opencv, nonneg_movie=True,\n",
    "                  border_nan=border_nan)\n",
    "\n",
    "mov=(cm.movie(mot_correct)).play(magnification=2, fr=reader.fps, q_min=0.1, q_max=99.9)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2c073dc3",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Make sure our parallel cluster is shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99671c6",
   "metadata": {},
   "source": [
    "cm.stop_server(dview=cluster)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
