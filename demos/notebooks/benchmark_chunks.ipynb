{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a6308f-0c88-446f-ba0e-f8ee2e745f48",
   "metadata": {},
   "source": [
    "## LBM : Benchmark Chunk Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10ec59-6f3d-4bad-9553-f4be07d02797",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False                                 # flag to re-extract tiffs with extracted data\n",
    "\n",
    "datapath = Path('/data2/fpo/data/raw/high_res/')   # string pointing to directory containing your data\n",
    "savepath = Path('/data2/fpo/data/extraction/')           # string pointing to directory containing your data\n",
    "savepath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "htiffs = [x for x in datapath.glob('*.tif')]      # this accumulates a list of every filepath which contains a .tif file\n",
    "reader = scanreader.read_scan(str(htiffs[0]), join_contiguous=True, lbm=True, x_cut=(6,6), y_cut=(17,0))  # this should take < 2s, no actual data is being read yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2ec29-aefa-41ea-95b4-01b17a7b87af",
   "metadata": {
    "execution": {
     "execution_failed": "2024-05-23T17:59:58.095Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "# Give this notebook access to the root package\n",
    "sys.path.append('../../')  # TODO: Take this out when we upload to pypi\n",
    "print(sys.path[0])\n",
    "\n",
    "import core.io\n",
    "import scanreader\n",
    "\n",
    "import zarr\n",
    "import bokeh.plotting as bpl\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "from IPython import get_ipython\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import dask.array as da\n",
    "    has_dask = True\n",
    "except ImportError:\n",
    "    has_dask = False\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except():\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "        get_ipython().run_line_magic('autoreload', '2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "bpl.output_notebook()\n",
    "hv.notebook_extension('bokeh')\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(format=\"{asctime} - {levelname} - [{filename} {funcName}() {lineno}] - pid {process} - {message}\",\n",
    "                    filename=None, \n",
    "                    level=logging.WARNING, style=\"{\") # this shows you just errors that can harm your program\n",
    "                    # level=logging.DEBUG, style=\"{\") # this shows you general information that developers use to trakc their program \n",
    "                    # (be careful when playing movies, there will be a lot of debug messages)\n",
    "\n",
    "# set env variables \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "\n",
    "# this session had output planes ordered differently, we need to reorder them\n",
    "chan_order = np.array([ 1, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14, 15, 16, 17, 3, 18, 19, 20, 21, 22, 23, 4, 24, 25, 26, 27, 28, 29, 30])  # this is specific to our dataset\n",
    "chan_order = [x-1 for x in chan_order]  # convert to 0-based indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd95b953-99bc-47fc-b995-bc7ec6f16cfc",
   "metadata": {},
   "source": [
    "# Benchmark: Chunk Sizes\n",
    "\n",
    "The below section demonstrates how to search for the optimal data chunking/partitioning scheme for our datasets.\n",
    "\n",
    "- Chunking by **Image**:\n",
    "\n",
    "  The smallest chunks we have. Each image is loaded in parallel, which requires many cores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54805353-9c74-475e-bbdb-605fdb6ebb61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T17:35:38.196314Z",
     "iopub.status.busy": "2024-05-22T17:35:38.195073Z",
     "iopub.status.idle": "2024-05-22T17:35:38.335561Z",
     "shell.execute_reply": "2024-05-22T17:35:38.334857Z",
     "shell.execute_reply.started": "2024-05-22T17:35:38.196139Z"
    }
   },
   "outputs": [],
   "source": [
    "# directory to save our benchmarks\n",
    "benchmarks_savedir = datapath / 'benchmarks'\n",
    "benchmarks_savedir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec12ca9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T15:50:41.914458Z",
     "start_time": "2024-05-07T15:50:41.833759Z"
    },
    "execution": {
     "iopub.status.busy": "2024-05-22T17:24:28.079305Z",
     "iopub.status.idle": "2024-05-22T17:24:28.079724Z",
     "shell.execute_reply": "2024-05-22T17:24:28.079526Z",
     "shell.execute_reply.started": "2024-05-22T17:24:28.079502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to process a dataset\n",
    "# Data can be any numpy or numpy-like array\n",
    "\n",
    "def benchmark_chunk_sizes(data, chunk_shape, savepath='', overwrite=True):\n",
    "    savepath = Path(savepath).with_suffix('.zarr')\n",
    "    \n",
    "    # benchmark write\n",
    "    start = time.time()\n",
    "    store = zarr.DirectoryStore(savepath)  # save data to persistent disk storage\n",
    "    z = zarr.zeros(data.shape, chunks=chunk_shape, dtype='int16', store=store, overwrite=overwrite)\n",
    "\n",
    "    if hasattr(data, 'compute'):\n",
    "        z[:] = data.compute()             # this will auto-chunk based on the specified chunks in 'open'\n",
    "    else:\n",
    "        z[:] = data\n",
    "\n",
    "    write = time.time() - start\n",
    "    formatted_write = f\"{write:.2f}\"\n",
    "\n",
    "    # benchmark read\n",
    "    start = time.time()\n",
    "    _ = z[:]\n",
    "    read = time.time() - start\n",
    "    formatted_read = f\"{read:.2f}\"\n",
    "\n",
    "    chunksize_nbytes = np.prod(chunk_shape) * z.dtype.itemsize  # 2 bytes per int16\n",
    "    return [\n",
    "        str(data.shape),\n",
    "        str(z.chunks),\n",
    "        z.nbytes / 1e6,\n",
    "        chunksize_nbytes / 1e6,\n",
    "        z.dtype,\n",
    "        z.order,\n",
    "        formatted_read,\n",
    "        formatted_write,\n",
    "        z.store.path\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e729ba-bc09-4c32-bec9-634c3c4be434",
   "metadata": {},
   "source": [
    "Use our raw dataset to get image shapes, and read/write operation on the **same dataset** with different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352eb260-2181-4a28-b57a-e269b141ef0b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T17:24:28.080455Z",
     "iopub.status.idle": "2024-05-22T17:24:28.080869Z",
     "shell.execute_reply": "2024-05-22T17:24:28.080670Z",
     "shell.execute_reply.started": "2024-05-22T17:24:28.080646Z"
    }
   },
   "outputs": [],
   "source": [
    "# we use dimensions from our initial raw data store\n",
    "zinf = zarr.open(save_str)\n",
    "zinf.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75598193",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T17:24:28.081565Z",
     "iopub.status.idle": "2024-05-22T17:24:28.081979Z",
     "shell.execute_reply": "2024-05-22T17:24:28.081783Z",
     "shell.execute_reply.started": "2024-05-22T17:24:28.081759Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'Array  [x,y,z,t]',\n",
    "    'Chunks [x,y,z,t]',\n",
    "    'Chunk Size (Mb)',\n",
    "    'Array Size (Mb)',\n",
    "    'Data Type',\n",
    "    'Order',\n",
    "    'Read Time (s)',\n",
    "    'Write Time (s)',\n",
    "    'Save Path'\n",
    "]\n",
    "\n",
    "# the chunk sizes we want to benchmark\n",
    "chunksizes = [\n",
    "    (zinf.shape[0], zinf.shape[1], 1, zinf.shape[3]),  # [300x300x1x1750]\n",
    "    (zinf.shape[0], zinf.shape[1], zinf.shape[2], 1),  # [300x300x30x1  ]\n",
    "    (zinf.shape[0], zinf.shape[1], 1, 1)               # [300x300x1x1   ]\n",
    "]\n",
    "\n",
    "# give our dataset a name. this will be the column header\n",
    "names = [\n",
    "    ('chunked_by_plane'),  # [300x300x1x1750]\n",
    "    ('chunked_by_frame'),  # [300x300x30x1  ]\n",
    "    ('chunked_by_image')   # [300x300x1x1   ]\n",
    "]\n",
    "\n",
    "benchmark_chunks_dir = benchmarks_savedir / \"chunks\"\n",
    "benchmark_chunks_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "vals = {data_name: [] for data_name in names}\n",
    "for i, (chunksize, dataset) in enumerate(zip(chunksizes, names)):\n",
    "    # save the same data but with different chunk sizes\n",
    "    chunks_save = benchmark_chunks_dir / f'{dataset}.zarr'\n",
    "    # new store \n",
    "    store = zarr.DirectoryStore(save_str)  # save data to persistent disk storage\n",
    "    z = zarr.zeros(zinf.shape, chunks=(zinf.shape[0], zinf.shape[1], 1, zinf.shape[3]), dtype='int16', store=store, overwrite=True)\n",
    "    vals[dataset] = benchmark_chunk_sizes(zinf, chunksize, savepath=f\"{chunks_save}\", overwrite=True)\n",
    "\n",
    "df = pd.DataFrame(index=labels, columns=names, data=None)\n",
    "for k, v in vals.items():\n",
    "    df[k] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f9c10-e444-4663-b7e8-aa32ada47cf5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T17:24:28.083034Z",
     "iopub.status.idle": "2024-05-22T17:24:28.083462Z",
     "shell.execute_reply": "2024-05-22T17:24:28.083255Z",
     "shell.execute_reply.started": "2024-05-22T17:24:28.083231Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
